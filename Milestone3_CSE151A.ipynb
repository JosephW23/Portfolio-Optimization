{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frg9Ccqp_aUf"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal in the data processing stage was to get the dataset as ready as possible for training models that depend on feature scaling and data quality, such as Ridge Regression and Linear Regression. To make sure our models would train on relevant data, we started by sifting and choosing pertinent columns to concentrate on the most important macroeconomic and financial variables. To ensure reliable trend analysis, we preserved the continuity and completeness of the time series data by pivoting the data and used forward filling to address missing values. Consistent scaling was guaranteed by engineering new characteristics and normalizing them with z-scores. This is important since algorithms such as Linear Regression rely on the assumption that input features are on a same scale. We were able to condense the total macroeconomic conditions into a single, more manageable goal variable by developing a composite indicator (MACRO_AVGZ). The robustness of the models is increased by downsampling to a monthly frequency, which reduced noise and recorded more steady trends. By dividing the dataset into training and test sets, we were able to assess the models' performance on previously unseen data, which gave us information on how well they could generalize. Together, these actions made sure that our models would train on clear, organized, and pertinent data, which improved their accuracy and capacity to generalize to new data. This was especially important for the Ridge Regression model, which uses regularization to reduce overfitting."
      ],
      "metadata": {
        "id": "vikrjmsD6IlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "OAW7x4ISe3DR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'JPMaQS_Quantamental_Indicators.csv'\n",
        "df = pd.read_csv(file_path, parse_dates=['real_date'])\n",
        "\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRlDV-Ycg1uF",
        "outputId": "61f92c34-80ad-4c5d-ea6c-8b68cb0171d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'real_date', 'cid', 'xcat', 'value', 'grading', 'eop_lag',\n",
            "       'mop_lag'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['cid', 'xcat', 'real_date', 'value', 'grading', 'eop_lag', 'mop_lag']]\n",
        "df['ticker'] = df['cid'] + \"_\" + df['xcat']"
      ],
      "metadata": {
        "id": "fvwE6Mn_hUzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-sections and categories of interest\n",
        "cids = [\"AUD\", \"CAD\", \"CHF\", \"EUR\", \"GBP\", \"JPY\", \"NOK\", \"NZD\", \"SEK\", \"USD\"]\n",
        "xcats = [\n",
        "    \"RYLDIRS05Y_NSA\", \"INTRGDPv5Y_NSA_P1M1ML12_3MMA\", \"CPIC_SJA_P6M6ML6AR\",\n",
        "    \"CPIH_SA_P1M1ML12\", \"INFTEFF_NSA\", \"PCREDITBN_SJA_P1M1ML12\",\n",
        "    \"RGDP_SA_P1Q1QL4_20QMA\"\n",
        "]\n",
        "df = df[df['cid'].isin(cids) & df['xcat'].isin(xcats)]"
      ],
      "metadata": {
        "id": "GoRkv8R2idFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate columns for each xcat\n",
        "df_pivot = df.pivot_table(index=['real_date', 'cid'], columns='xcat', values='value').reset_index()\n",
        "\n",
        "print(df_pivot.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LGp23z_lLt9",
        "outputId": "94b09f93-7596-4038-c870-583b3390673f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xcat  real_date  cid  CPIC_SJA_P6M6ML6AR  CPIH_SA_P1M1ML12  INFTEFF_NSA  \\\n",
            "0    2000-01-03  AUD            1.428580          1.647446     1.874567   \n",
            "1    2000-01-03  CAD            1.709066          2.292576     1.749144   \n",
            "2    2000-01-03  CHF                 NaN          1.663356     0.827757   \n",
            "3    2000-01-03  EUR                 NaN          1.446079          NaN   \n",
            "4    2000-01-03  GBP            0.314695          1.156351     2.005796   \n",
            "\n",
            "xcat  INTRGDPv5Y_NSA_P1M1ML12_3MMA  PCREDITBN_SJA_P1M1ML12  \\\n",
            "0                         0.247776                9.517471   \n",
            "1                         1.788620                6.888624   \n",
            "2                         0.381352                4.423255   \n",
            "3                              NaN                     NaN   \n",
            "4                        -0.108668                     NaN   \n",
            "\n",
            "xcat  RGDP_SA_P1Q1QL4_20QMA  RYLDIRS05Y_NSA  \n",
            "0                  4.307267        5.391185  \n",
            "1                  3.303088        4.637855  \n",
            "2                  1.156352        2.362124  \n",
            "3                       NaN             NaN  \n",
            "4                  2.666125        5.138370  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values\n",
        "df_pivot.sort_values(by=['cid', 'real_date'], inplace=True)\n",
        "df_pivot.fillna(method='ffill', inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_mw1dp8lQuQ",
        "outputId": "575700ee-553f-434d-e232-78e9c88b1b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-06e6d2f9dd1a>:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_pivot.fillna(method='ffill', inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate signal constituents\n",
        "df_pivot['XGDP_NEG'] = -df_pivot['INTRGDPv5Y_NSA_P1M1ML12_3MMA']\n",
        "df_pivot['XCPI_NEG'] = - (df_pivot['CPIC_SJA_P6M6ML6AR'] + df_pivot['CPIH_SA_P1M1ML12']) / 2 + df_pivot['INFTEFF_NSA']\n",
        "df_pivot['XPCG_NEG'] = - df_pivot['PCREDITBN_SJA_P1M1ML12'] + df_pivot['INFTEFF_NSA'] + df_pivot['RGDP_SA_P1Q1QL4_20QMA']"
      ],
      "metadata": {
        "id": "8LL0B9xJlT_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevant columns\n",
        "selected_cols = ['real_date', 'cid', 'XGDP_NEG', 'XCPI_NEG', 'XPCG_NEG', 'RYLDIRS05Y_NSA']\n",
        "df_pivot = df_pivot[selected_cols]"
      ],
      "metadata": {
        "id": "Zjd-e4JNlXXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize feature variables using z-scores\n",
        "for col in ['XGDP_NEG', 'XCPI_NEG', 'XPCG_NEG', 'RYLDIRS05Y_NSA']:\n",
        "    df_pivot[col + '_ZN4'] = (df_pivot[col] - df_pivot[col].mean()) / df_pivot[col].std()"
      ],
      "metadata": {
        "id": "HLW_kZ-XlZtC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f810409e-bdaa-47b7-97bc-5e2018ac1d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-dd9c25a2a591>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_pivot[col + '_ZN4'] = (df_pivot[col] - df_pivot[col].mean()) / df_pivot[col].std()\n",
            "<ipython-input-9-dd9c25a2a591>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_pivot[col + '_ZN4'] = (df_pivot[col] - df_pivot[col].mean()) / df_pivot[col].std()\n",
            "<ipython-input-9-dd9c25a2a591>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_pivot[col + '_ZN4'] = (df_pivot[col] - df_pivot[col].mean()) / df_pivot[col].std()\n",
            "<ipython-input-9-dd9c25a2a591>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_pivot[col + '_ZN4'] = (df_pivot[col] - df_pivot[col].mean()) / df_pivot[col].std()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Composite indicator\n",
        "df_pivot['MACRO_AVGZ'] = df_pivot[['XGDP_NEG_ZN4', 'XCPI_NEG_ZN4', 'XPCG_NEG_ZN4', 'RYLDIRS05Y_NSA_ZN4']].mean(axis=1)"
      ],
      "metadata": {
        "id": "9wE4rB33lfcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monthly frequency\n",
        "df_pivot.set_index('real_date', inplace=True)\n",
        "df_monthly = df_pivot.resample('M').last()"
      ],
      "metadata": {
        "id": "cdn5WCGDlgwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values\n",
        "df_monthly.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "TKH8v_SylvKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features and target\n",
        "X = df_monthly[['XGDP_NEG_ZN4', 'XCPI_NEG_ZN4', 'XPCG_NEG_ZN4', 'RYLDIRS05Y_NSA_ZN4']]\n",
        "y = df_monthly['MACRO_AVGZ']"
      ],
      "metadata": {
        "id": "dJ2EsZzElwyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ripVD9n3ljUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and feature expansion\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), X.columns)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "jAn6YJfXl1PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building our Model"
      ],
      "metadata": {
        "id": "x6-8ctL0e0M9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1"
      ],
      "metadata": {
        "id": "U5W_v5OCEZcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression pipeline\n",
        "linear_model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_expansion', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('model', LinearRegression())\n",
        "])"
      ],
      "metadata": {
        "id": "LJSGcF-Me2us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression model\n",
        "linear_model_pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "knQ3_xlrl6pn",
        "outputId": "c77ac42b-5f80-4669-dca8-48911c7b7b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('preprocessor',\n",
              "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
              "                                                  Index(['XGDP_NEG_ZN4', 'XCPI_NEG_ZN4', 'XPCG_NEG_ZN4', 'RYLDIRS05Y_NSA_ZN4'], dtype='object', name='xcat'))])),\n",
              "                ('feature_expansion', PolynomialFeatures(include_bias=False)),\n",
              "                ('model', LinearRegression())])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                                  Index([&#x27;XGDP_NEG_ZN4&#x27;, &#x27;XCPI_NEG_ZN4&#x27;, &#x27;XPCG_NEG_ZN4&#x27;, &#x27;RYLDIRS05Y_NSA_ZN4&#x27;], dtype=&#x27;object&#x27;, name=&#x27;xcat&#x27;))])),\n",
              "                (&#x27;feature_expansion&#x27;, PolynomialFeatures(include_bias=False)),\n",
              "                (&#x27;model&#x27;, LinearRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                                  Index([&#x27;XGDP_NEG_ZN4&#x27;, &#x27;XCPI_NEG_ZN4&#x27;, &#x27;XPCG_NEG_ZN4&#x27;, &#x27;RYLDIRS05Y_NSA_ZN4&#x27;], dtype=&#x27;object&#x27;, name=&#x27;xcat&#x27;))])),\n",
              "                (&#x27;feature_expansion&#x27;, PolynomialFeatures(include_bias=False)),\n",
              "                (&#x27;model&#x27;, LinearRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                 Index([&#x27;XGDP_NEG_ZN4&#x27;, &#x27;XCPI_NEG_ZN4&#x27;, &#x27;XPCG_NEG_ZN4&#x27;, &#x27;RYLDIRS05Y_NSA_ZN4&#x27;], dtype=&#x27;object&#x27;, name=&#x27;xcat&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;XGDP_NEG_ZN4&#x27;, &#x27;XCPI_NEG_ZN4&#x27;, &#x27;XPCG_NEG_ZN4&#x27;, &#x27;RYLDIRS05Y_NSA_ZN4&#x27;], dtype=&#x27;object&#x27;, name=&#x27;xcat&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures(include_bias=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_linear = linear_model_pipeline.predict(X_train)\n",
        "y_test_pred_linear = linear_model_pipeline.predict(X_test)\n",
        "\n",
        "train_mse_linear = mean_squared_error(y_train, y_train_pred_linear)\n",
        "test_mse_linear = mean_squared_error(y_test, y_test_pred_linear)\n",
        "train_r2_linear = r2_score(y_train, y_train_pred_linear)\n",
        "test_r2_linear = r2_score(y_test, y_test_pred_linear)\n",
        "\n",
        "print(\"Linear Regression:\")\n",
        "print(f\"Training MSE: {train_mse_linear}\")\n",
        "print(f\"Test MSE: {test_mse_linear}\")\n",
        "print(f\"Training R²: {train_r2_linear}\")\n",
        "print(f\"Test R²: {test_r2_linear}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk5I0dDbiO3G",
        "outputId": "bb9774b3-12a3-43c0-9baa-cb05762efd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression:\n",
            "Training MSE: 2.6444872105683483e-31\n",
            "Test MSE: 1.1537005234208904e-31\n",
            "Training R²: 1.0\n",
            "Test R²: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the findings of the linear regression, the model's Mean Squared Error (MSE) for the training set (2.6444872105683483e-31) and the test set (1.1537005234208904e-31) is remarkably low, meaning that the values in both datasets are nearly similar. For both the training and test sets, the R2 value is 1.0, which indicates that the model fully explains all of the variance in the target variable. These apparently perfect results point to a serious problem: overfitting. Overfitting happens when the model performs exceptionally well on the training set but may perform poorly when applied to fresh, unseen data because it catches both the noise and outliers in addition to the underlying patterns in the training data.\n",
        "According to the findings of the linear regression, the model's Mean Squared Error (MSE) for the training set (2.6444872105683483e-31) and the test set (1.1537005234208904e-31) is remarkably low, meaning that the values in both datasets are nearly similar. For both the training and test sets, the R2 value is 1.0, which indicates that the model fully explains all of the variance in the target variable. These apparently perfect results point to a serious problem: overfitting. Overfitting happens when the model performs exceptionally well on the training set but may perform poorly when applied to fresh, unseen data because it catches both the noise and outliers in addition to the underlying patterns in the training data."
      ],
      "metadata": {
        "id": "ydXbJPrt76Uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2"
      ],
      "metadata": {
        "id": "GumsRF-_Ed-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "uEo-r9wU8WCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge Regression pipeline\n",
        "ridge_model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_expansion', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    ('model', Ridge(alpha=1.0))  # alpha is the regularization strength\n",
        "])"
      ],
      "metadata": {
        "id": "n060Bd8e7ifB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge Regression model\n",
        "ridge_model_pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "UucvY-cY7lSq",
        "outputId": "045a10fe-062f-4baf-f7cf-db53b89589a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('preprocessor',\n",
              "                 ColumnTransformer(transformers=[('num', StandardScaler(),\n",
              "                                                  Index(['XGDP_NEG_ZN4', 'XCPI_NEG_ZN4', 'XPCG_NEG_ZN4', 'RYLDIRS05Y_NSA_ZN4'], dtype='object', name='xcat'))])),\n",
              "                ('feature_expansion', PolynomialFeatures(include_bias=False)),\n",
              "                ('model', Ridge())])"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                                  Index([&#x27;XGDP_NEG_ZN4&#x27;, &#x27;XCPI_NEG_ZN4&#x27;, &#x27;XPCG_NEG_ZN4&#x27;, &#x27;RYLDIRS05Y_NSA_ZN4&#x27;], dtype=&#x27;object&#x27;, name=&#x27;xcat&#x27;))])),\n",
              "                (&#x27;feature_expansion&#x27;, PolynomialFeatures(include_bias=False)),\n",
              "                (&#x27;model&#x27;, Ridge())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                                  Index([&#x27;XGDP_NEG_ZN4&#x27;, &#x27;XCPI_NEG_ZN4&#x27;, &#x27;XPCG_NEG_ZN4&#x27;, &#x27;RYLDIRS05Y_NSA_ZN4&#x27;], dtype=&#x27;object&#x27;, name=&#x27;xcat&#x27;))])),\n",
              "                (&#x27;feature_expansion&#x27;, PolynomialFeatures(include_bias=False)),\n",
              "                (&#x27;model&#x27;, Ridge())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;, StandardScaler(),\n",
              "                                 Index([&#x27;XGDP_NEG_ZN4&#x27;, &#x27;XCPI_NEG_ZN4&#x27;, &#x27;XPCG_NEG_ZN4&#x27;, &#x27;RYLDIRS05Y_NSA_ZN4&#x27;], dtype=&#x27;object&#x27;, name=&#x27;xcat&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;XGDP_NEG_ZN4&#x27;, &#x27;XCPI_NEG_ZN4&#x27;, &#x27;XPCG_NEG_ZN4&#x27;, &#x27;RYLDIRS05Y_NSA_ZN4&#x27;], dtype=&#x27;object&#x27;, name=&#x27;xcat&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PolynomialFeatures</label><div class=\"sk-toggleable__content\"><pre>PolynomialFeatures(include_bias=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_pred_ridge = ridge_model_pipeline.predict(X_train)\n",
        "y_test_pred_ridge = ridge_model_pipeline.predict(X_test)\n",
        "\n",
        "train_mse_ridge = mean_squared_error(y_train, y_train_pred_ridge)\n",
        "test_mse_ridge = mean_squared_error(y_test, y_test_pred_ridge)\n",
        "train_r2_ridge = r2_score(y_train, y_train_pred_ridge)\n",
        "test_r2_ridge = r2_score(y_test, y_test_pred_ridge)\n",
        "\n",
        "print(\"Ridge Regression:\")\n",
        "print(f\"Training MSE: {train_mse_ridge}\")\n",
        "print(f\"Test MSE: {test_mse_ridge}\")\n",
        "print(f\"Training R²: {train_r2_ridge}\")\n",
        "print(f\"Test R²: {test_r2_ridge}\")\n",
        "\n",
        "cv_scores_ridge = cross_val_score(ridge_model_pipeline, X, y, cv=5, scoring='r2')\n",
        "print(\"Cross-Validation R² Scores:\", cv_scores_ridge)\n",
        "print(\"Mean Cross-Validation R²:\", np.mean(cv_scores_ridge))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QweDoLJ87l9Y",
        "outputId": "fece7f09-661e-4c74-a9a2-625d28c61aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Regression:\n",
            "Training MSE: 1.453966772964182e-05\n",
            "Test MSE: 1.4214579694990766e-05\n",
            "Training R²: 0.9999533961393585\n",
            "Test R²: 0.9999530193010754\n",
            "Cross-Validation R² Scores: [0.99963664 0.99984517 0.99967164 0.99912687 0.99812884]\n",
            "Mean Cross-Validation R²: 0.9992818331772734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a test MSE of 1.4214579694990766e-05 and a training MSE of 1.453966772964182e-05, the Ridge Regression model performs exceptionally well, showing remarkably accurate predictions for both training and test datasets. The training and test sets' R2 values, which are 0.9999530193010754 and 0.9999533961393585, respectively, are incredibly high and indicate that the model explains almost all of the variance in the target variable. Moreover, the cross-validation R2 scores demonstrate strong and consistent performance across several data subsets, with a mean of 0.9992818331772734 and a range of 0.99812884 to 0.99984517. These findings demonstrate how Ridge Regression achieves better generalization and successfully reduces overfitting, a problem with the original Linear Regression model. As a result, Ridge Regression balances good fit and predictive capacity on both known and unknown data, demonstrating its high degree of accuracy and dependability in predicting the composite financial indicator in the portfolio optimization."
      ],
      "metadata": {
        "id": "ahxxog_s9ICJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer to Questions"
      ],
      "metadata": {
        "id": "VRqz39n6Ejon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer the questions: Where does your model fit in the fitting graph? and What are the next models you are thinking of and why?\n",
        "\n",
        "Question 1:\n",
        "\n",
        "Linear Regression:The initial excellent R2 values of the Linear Regression model suggested that it was overfitting. This implies that it fits the training data—noise included—too closely, which may result in inadequate generalization to fresh data. This model would be on the right side of the fitting graph, where model complexity is high and overfitting causes the error on test data to start rising.\n",
        "\n",
        "Ridge Regression:The appropriate range for model complexity is fit by the Ridge Regression model. By penalizing large coefficients, the regularization term helps prevent overfitting and achieve a balance between variance and bias. Good generalization is demonstrated by this model's low MSEs and excellent R2 scores on both test and training sets of data. Ridge Regression would be close to the bottom of the U-shaped curve in the fitting graph, where test and training errors are minimized and optimal model complexity is represented.\n",
        "\n",
        "To compare the effects of regularization, we employed Ridge Regression and Linear Regression:A baseline for understanding the performance of a basic model in the absence of regularization was provided by linear regression. Its flawless R2 scores demonstrated that it assisted in identifying any overfitting problems.In order to solve the overfitting seen with Linear Regression, Ridge Regression was devised. Ridge Regression penalizes big coefficients, which lowers overfitting and enhances generalization to fresh data by including an L2 regularization factor. This illustrated how crucial regularization is to building a strong prediction model.\n",
        "\n",
        "Question 2:\n",
        "Investigating non-linear models, such as the Random Forest Regressor, can be very helpful for portfolio optimization in the following stages. This is the reason why: The Random Forest Regressor to enhance predictive performance, Random Forest is an ensemble learning technique that combines several decision trees. It records intricate interactions and non-linear correlations between features that may be overlooked by linear models. These kinds of associations are common in financial data, thus this can be quite helpful there.Advantages: By averaging several trees, it lessens overfitting and is resistant to noise and outliers in the data. It also offers feature importance metrics, which are helpful for comprehending the underlying causes of predictions, and manages both numerical and categorical characteristics with ease. Comparison with Linear Models: We can assess if incorporating non-linear interactions considerably increases the prediction accuracy and robustness of the portfolio optimization model by contrasting Random Forest's performance with that of the linear models (Linear and Ridge Regression).In situations when there are intricate relationships between the financial indicators, Random Forest may perform better, providing a possibly more accurate and trustworthy model for making decisions."
      ],
      "metadata": {
        "id": "ndbed7_W9Qia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion section: What is the conclusion of your 1st model? What can be done to possibly improve it?\n",
        "For both the training and test datasets, the Linear Regression model showed flawless R2 values, suggesting a significant level of overfitting. This flawless fit implies that the training data's noise and particular patterns, which are not very generalizable to fresh, unobserved data, were being captured by the model. Many approaches can be taken into consideration in order to enhance the Linear Regression model. Large coefficients can be penalized and overfitting can be decreased by using regularization techniques like Lasso Regression (L1 regularization) and Ridge Regression (L2 regularization). Furthermore, the model can be made simpler and more capable of generalization by limiting the number of features to just those that are most pertinent. Furthermore, overfitting can be lessened by evaluating the model's performance using cross-validation techniques and adjusting the hyperparameters accordingly.\n",
        "\n",
        "In contrast to Linear Regression, however, the Ridge Regression model offered a more robust and balanced match. For the training and test datasets, it showed low MSE values and good R2 scores. The regularization term's addition reduced overfitting and produced a model that performs well when applied to new data. The robustness and dependability of the model were further validated by the cross-validation scores. Still, there may be room for advancement with the Ridge Regression model. Finding the ideal regularization parameter (alpha) value that reduces error and improves generalization can be accomplished by further fine-tuning it using methods like Grid Search or Random Search. Furthermore, enhancing or adding new features might help the model forecast more accurately by capturing more pertinent data.Exploring ensemble methods such as Random Forest or Gradient Boosting can capture more complex relatio\n",
        "\n",
        "There are various ways to build on the Ridge Regression model's performance. Using models such as the Random Forest Regressor can aid in identifying non-linear patterns within the data. Model performance can be further improved by experimenting with different regularization strategies, such as ElasticNet, which combines L1 and L2 regularization. Robust and trustworthy predictions for portfolio optimization can be achieved by regularly assessing the performance of the model using cross-validation and modifying the modeling strategy in response to the findings. By following these procedures, we may create a more resilient model that fits the training set of data more accurately and generalizes to new data with greater efficacy, offering more trustworthy insights for portfolio optimization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "29niOlMkAOWT"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "collapsed_sections": [
        "x6-8ctL0e0M9"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}