{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frg9Ccqp_aUf"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "OAW7x4ISe3DR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'JPMaQS_Quantamental_Indicators.csv'\n",
        "df = pd.read_csv(file_path, parse_dates=['real_date'])\n",
        "\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRlDV-Ycg1uF",
        "outputId": "9bdb9e94-30e7-44d7-c30b-2313adb47465"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'real_date', 'cid', 'xcat', 'value', 'grading', 'eop_lag',\n",
            "       'mop_lag'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['cid', 'xcat', 'real_date', 'value', 'grading', 'eop_lag', 'mop_lag']]\n",
        "df['ticker'] = df['cid'] + \"_\" + df['xcat']"
      ],
      "metadata": {
        "id": "fvwE6Mn_hUzc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-sections and categories of interest\n",
        "cids = [\"AUD\", \"CAD\", \"CHF\", \"EUR\", \"GBP\", \"JPY\", \"NOK\", \"NZD\", \"SEK\", \"USD\"]\n",
        "xcats = [\n",
        "    \"RYLDIRS05Y_NSA\", \"INTRGDPv5Y_NSA_P1M1ML12_3MMA\", \"CPIC_SJA_P6M6ML6AR\",\n",
        "    \"CPIH_SA_P1M1ML12\", \"INFTEFF_NSA\", \"PCREDITBN_SJA_P1M1ML12\",\n",
        "    \"RGDP_SA_P1Q1QL4_20QMA\"\n",
        "]\n",
        "df = df[df['cid'].isin(cids) & df['xcat'].isin(xcats)]"
      ],
      "metadata": {
        "id": "GoRkv8R2idFz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate columns for each xcat\n",
        "df_pivot = df.pivot_table(index=['real_date', 'cid'], columns='xcat', values='value').reset_index()\n",
        "\n",
        "print(df_pivot.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LGp23z_lLt9",
        "outputId": "b7d668bf-fd58-4e7f-8d6c-77e6e660dd9c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xcat  real_date  cid  CPIC_SJA_P6M6ML6AR  CPIH_SA_P1M1ML12  INFTEFF_NSA  \\\n",
            "0    2000-01-03  AUD            1.428580          1.647446     1.874567   \n",
            "1    2000-01-03  CAD            1.709066          2.292576     1.749144   \n",
            "2    2000-01-03  CHF                 NaN          1.663356     0.827757   \n",
            "3    2000-01-03  EUR                 NaN          1.446079          NaN   \n",
            "4    2000-01-03  GBP            0.314695          1.156351     2.005796   \n",
            "\n",
            "xcat  INTRGDPv5Y_NSA_P1M1ML12_3MMA  PCREDITBN_SJA_P1M1ML12  \\\n",
            "0                         0.247776                9.517471   \n",
            "1                         1.788620                6.888624   \n",
            "2                         0.381352                4.423255   \n",
            "3                              NaN                     NaN   \n",
            "4                        -0.108668                     NaN   \n",
            "\n",
            "xcat  RGDP_SA_P1Q1QL4_20QMA  RYLDIRS05Y_NSA  \n",
            "0                  4.307267        5.391185  \n",
            "1                  3.303088        4.637855  \n",
            "2                  1.156352        2.362124  \n",
            "3                       NaN             NaN  \n",
            "4                  2.666125        5.138370  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values\n",
        "df_pivot.sort_values(by=['cid', 'real_date'], inplace=True)\n",
        "df_pivot.fillna(method='ffill', inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_mw1dp8lQuQ",
        "outputId": "b18c8267-e526-43fa-d7ce-e515ccb51ae5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-0302210b554c>:3: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_pivot.fillna(method='ffill', inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Signal constituents\n",
        "df_pivot['XGDP_NEG'] = -df_pivot['INTRGDPv5Y_NSA_P1M1ML12_3MMA']\n",
        "df_pivot['XCPI_NEG'] = - (df_pivot['CPIC_SJA_P6M6ML6AR'] + df_pivot['CPIH_SA_P1M1ML12']) / 2 + df_pivot['INFTEFF_NSA']\n",
        "df_pivot['XPCG_NEG'] = - df_pivot['PCREDITBN_SJA_P1M1ML12'] + df_pivot['INFTEFF_NSA'] + df_pivot['RGDP_SA_P1Q1QL4_20QMA']"
      ],
      "metadata": {
        "id": "8LL0B9xJlT_Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Relevant columns\n",
        "selected_cols = ['real_date', 'cid', 'XGDP_NEG', 'XCPI_NEG', 'XPCG_NEG', 'RYLDIRS05Y_NSA']\n",
        "df_pivot = df_pivot[selected_cols]"
      ],
      "metadata": {
        "id": "Zjd-e4JNlXXA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize feature variables using z-scores\n",
        "for col in ['XGDP_NEG', 'XCPI_NEG', 'XPCG_NEG', 'RYLDIRS05Y_NSA']:\n",
        "    df_pivot[col + '_ZN4'] = (df_pivot[col] - df_pivot[col].mean()) / df_pivot[col].std()"
      ],
      "metadata": {
        "id": "HLW_kZ-XlZtC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Composite indicator\n",
        "df_pivot['MACRO_AVGZ'] = df_pivot[['XGDP_NEG_ZN4', 'XCPI_NEG_ZN4', 'XPCG_NEG_ZN4', 'RYLDIRS05Y_NSA_ZN4']].mean(axis=1)"
      ],
      "metadata": {
        "id": "9wE4rB33lfcM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monthly frequency\n",
        "df_pivot.set_index('real_date', inplace=True)\n",
        "df_monthly = df_pivot.resample('M').last()"
      ],
      "metadata": {
        "id": "cdn5WCGDlgwC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values\n",
        "df_monthly.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "TKH8v_SylvKf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features and target\n",
        "X = df_monthly[['XGDP_NEG_ZN4', 'XCPI_NEG_ZN4', 'XPCG_NEG_ZN4', 'RYLDIRS05Y_NSA_ZN4']]\n",
        "y = df_monthly['MACRO_AVGZ']"
      ],
      "metadata": {
        "id": "dJ2EsZzElwyF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ripVD9n3ljUo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling and feature expansion\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), X.columns)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "jAn6YJfXl1PS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building our Model"
      ],
      "metadata": {
        "id": "x6-8ctL0e0M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score"
      ],
      "metadata": {
        "id": "fDeiymkUJlro"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = StandardScaler()\n",
        "\n",
        "# Random Forest Regressor pipeline\n",
        "rf_model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "])"
      ],
      "metadata": {
        "id": "NMM8n7i7JoTD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Random Forest Regressor model\n",
        "rf_model_pipeline.fit(X_train, y_train)\n",
        "rf_predictions_train = rf_model_pipeline.predict(X_train)\n",
        "rf_predictions_test = rf_model_pipeline.predict(X_test)\n",
        "\n",
        "train_mse_rf = mean_squared_error(y_train, rf_predictions_train)\n",
        "test_mse_rf = mean_squared_error(y_test, rf_predictions_test)\n",
        "train_r2_rf = r2_score(y_train, rf_predictions_train)\n",
        "test_r2_rf = r2_score(y_test, rf_predictions_test)\n",
        "\n",
        "print(\"Random Forest Regressor:\")\n",
        "print(f\"Training MSE: {train_mse_rf}\")\n",
        "print(f\"Test MSE: {test_mse_rf}\")\n",
        "print(f\"Training R²: {train_r2_rf}\")\n",
        "print(f\"Test R²: {test_r2_rf}\")\n",
        "\n",
        "cv_scores_rf = cross_val_score(rf_model_pipeline, X, y, cv=5, scoring='r2')\n",
        "print(\"Cross-Validation R² Scores:\", cv_scores_rf)\n",
        "print(\"Mean Cross-Validation R²:\", np.mean(cv_scores_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRI8lYqNJyM9",
        "outputId": "f65b5908-fb2b-46ff-ecee-f8b4f87eaf69"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor:\n",
            "Training MSE: 0.0015991635940493449\n",
            "Test MSE: 0.0038223955048743608\n",
            "Training R²: 0.9932320517075023\n",
            "Test R²: 0.9808777373054388\n",
            "Cross-Validation R² Scores: [-2.78935438  0.28107945  0.84705524 -0.49929551 -0.19701962]\n",
            "Mean Cross-Validation R²: -0.4715069631131919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can attempt the following actions to reduce overfitting and enhance the Random Forest Regressor's performance. Hyperparameter tuning can adjust the Random Forest model's hyperparameters via methods like Grid Search or Random Search.To get rid of noise, fewer characteristics are used.\n",
        "As regularization take into account models with integrated regularization, such as Gradient Boosting.Employing stronger cross-validation methods is recommended."
      ],
      "metadata": {
        "id": "YGvBc1iEM2k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "IDgHXW3gMeT2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'model__n_estimators': [50, 100, 200],\n",
        "    'model__max_depth': [None, 10, 20, 30],\n",
        "    'model__min_samples_split': [2, 5, 10],\n",
        "    'model__min_samples_leaf': [1, 2, 4],\n",
        "    'model__bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Set up the pipeline\n",
        "rf_model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', RandomForestRegressor(random_state=42))\n",
        "])"
      ],
      "metadata": {
        "id": "2G2acHDZMfWe"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_model_pipeline, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, scoring='r2', verbose=2)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best parameters found: \", grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilue9it-Mjs2",
        "outputId": "ac99bb03-90ec-41e4-84e4-8faf9a0b0bab"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "Best parameters found:  {'model__bootstrap': True, 'model__max_depth': None, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'model__n_estimators': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "best_rf_predictions_train = best_rf_model.predict(X_train)\n",
        "best_rf_predictions_test = best_rf_model.predict(X_test)\n",
        "\n",
        "train_mse_best_rf = mean_squared_error(y_train, best_rf_predictions_train)\n",
        "test_mse_best_rf = mean_squared_error(y_test, best_rf_predictions_test)\n",
        "train_r2_best_rf = r2_score(y_train, best_rf_predictions_train)\n",
        "test_r2_best_rf = r2_score(y_test, best_rf_predictions_test)\n",
        "\n",
        "print(\"Best Random Forest Regressor:\")\n",
        "print(f\"Training MSE: {train_mse_best_rf}\")\n",
        "print(f\"Test MSE: {test_mse_best_rf}\")\n",
        "print(f\"Training R²: {train_r2_best_rf}\")\n",
        "print(f\"Test R²: {test_r2_best_rf}\")\n",
        "\n",
        "cv_scores_best_rf = cross_val_score(best_rf_model, X, y, cv=5, scoring='r2')\n",
        "print(\"Cross-Validation R² Scores:\", cv_scores_best_rf)\n",
        "print(\"Mean Cross-Validation R²:\", np.mean(cv_scores_best_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zSxvy_SMmgD",
        "outputId": "7f0393c8-718f-4572-bdf4-a754da2ccb5a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Random Forest Regressor:\n",
            "Training MSE: 0.0014175949461399484\n",
            "Test MSE: 0.003910450525903866\n",
            "Training R²: 0.9940004829206452\n",
            "Test R²: 0.9804372252648731\n",
            "Cross-Validation R² Scores: [-2.64822255  0.2695247   0.86733604 -0.49947904 -0.18997028]\n",
            "Mean Cross-Validation R²: -0.4401622252770364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a tendency towards overfitting as the Random Forest Regressor model is positioned more towards the right side of the graph. The extremely high training R2 score of 0.9940, which slightly decreases to 0.9804 for the test R2, and the notable differences in cross-validation R2 values demonstrate this. These indications point to the possibility that the model is underfitting to fresh data and overfitting to the training set. The next models being examined are the XGBoost (Extreme Gradient Boosting) and Gradient Boosting Regressor, based on the performance of the current model. Because it can handle non-linear relationships and interactions well and incorporate regularization to avoid overfitting, the Gradient Boosting Regressor is the model of choice. It constructs trees in a sequential manner, enabling each tree to fix the mistakes of the one before it.XGBoost, an advanced implementation of gradient boosting, is also considered due to its optimization for speed and performance, including regularization terms to control overfitting, making it highly effective in various data science competitions."
      ],
      "metadata": {
        "id": "gGt_kLFGPo4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer the Questions for Random Forest Regressor"
      ],
      "metadata": {
        "id": "FQtfhrkZP2ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question  Where does your model fit in the fitting graph?\n",
        "\n",
        "The Random Forest Regressor model leans toward the overfitting region and fits to the right side of the fitting graph. The high training R2 value (0.9940) and the declining test R2 value (0.9804) make this clear. The model is overfitting because of the fluctuation in the cross-validation R2 scores, which further suggests that the model is not generalizing well across various data subsets.\n",
        "\n"
      ],
      "metadata": {
        "id": "nwNGU-nlTWNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion Section for Random Forest Regressor\n"
      ],
      "metadata": {
        "id": "CXyfOFP6U1pe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting was seen in the Random Forest Regressor, which had a very high training R2 (0.9940) and a poor test R2 (0.9804). The model's inability to generalize effectively across various data subsets is demonstrated by the wide variations in cross-validation R2 scores. This shows that even though the model is capable of capturing intricate non-linear correlations in the training set, it might not function as well in the absence of data.There are multiple ways to enhance the model. First, to determine the ideal parameters for the Random Forest model, a thorough hyperparameter tuning can be carried out using GridSearchCV or RandomizedSearchCV. Second, by using feature engineering, one can produce new features—such as interaction terms, polynomial features, or domain-specific transformations—that more accurately represent the underlying patterns in the data. Third, overfitting can be avoided by using models like XGBoost or Gradient Boosting that include built-in regularization. Fourth, the model's capacity for generalization can be enhanced by utilizing strong cross-validation methods like stratified k-fold to guarantee that it is assessed on a variety of data subsets. Fifth, to improve overall performance, take into account ensemble methods like stacking or blending, which combine the strengths of various models. Finally, adding extra data to the model's training set can aid it."
      ],
      "metadata": {
        "id": "OeJfxe90U8h4"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}